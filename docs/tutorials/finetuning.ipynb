{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use `turkish-lm-tuner` to finetune an encoder-decoder language model on summarization task\n",
    "\n",
    "`turkish-lm-tuner` is a library designed for finetuning a language model on specific datasets. It is based on [transformers](https://github.com/huggingface/transformers/). It is designed to be easy to use and to work with any encoder and encoder-decoder language model that is supported by `transformers`. \n",
    "\n",
    "`turkish-lm-tuner` supports both encoder and encoder-decoder models. It offers wrappers for various task datasets (like paraphrasing, text classification, summarization, etc.).  Additionally, it includes easily importable and usable evaluation metrics for various tasks.\n",
    "\n",
    "In this tutorial, we will show how to use `turkish-lm-tuner` to finetune an encoder-decoder language model on summarization task. We will use [TR News dataset](https://doi.org/10.1007/s10579-021-09568-y) for fine-tuning [TURNA]() model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The library can be installed via pip:\n",
    "\n",
    "```bash\n",
    "pip install turkish-lm-tuner\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and processing dataset\n",
    "\n",
    "The library includes wrappers for various datasets. These wrappers can be used to import the dataset and to preprocess it for finetuning.\n",
    "\n",
    "For summarization task, we will use [TR News dataset](https://doi.org/10.1007/s10579-021-09568-y). This dataset includes news articles and their summaries. We will use the wrapper for this dataset to import and preprocess it based on the task configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from turkish_lm_tuner import DatasetProcessor\n",
    "\n",
    "dataset_name = \"tr_news\" \n",
    "task = \"summarization\"\n",
    "task_mode = '' # either '', '[NLU]', '[NLG]', '[S2S]'\n",
    "task_format=\"conditional_generation\"\n",
    "model_name = \"boun-tabi-lmt/TURNA\"\n",
    "max_input_length = 764\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "dataset_processor = DatasetProcessor(\n",
    "        dataset_name=dataset_name, task=task, task_format=task_format, task_mode=task_mode,\n",
    "        tokenizer_name=model_name, max_input_length=max_input_length, max_target_length=max_target_length\n",
    ")\n",
    "\n",
    "train_dataset = dataset_processor.load_and_preprocess_data(split='train')\n",
    "eval_dataset = dataset_processor.load_and_preprocess_data(split='validation')\n",
    "test_dataset = dataset_processor.load_and_preprocess_data(split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing finetuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "training_params = {\n",
    "    'num_train_epochs': 10\n",
    "    'per_device_train_batch_size': 4,\n",
    "    'per_device_eval_batch_size': 4,\n",
    "    'output_dir': './', \n",
    "    'evaluation_strategy': 'epoch',\n",
    "    'save_strategy': 'epoch',\n",
    "    'predict_with_generate': True    \n",
    "}\n",
    "optimizer_params = {\n",
    "    'optimizer_type': 'adafactor',\n",
    "    'scheduler': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_trainer = TrainerForConditionalGeneration(\n",
    "    model_name=model_name, task=task,\n",
    "    optimizer_params=optimizer_params,\n",
    "    training_params=training_params,\n",
    "    model_save_path=\"turna_summarization_tr_news\",\n",
    "    max_input_length=max_input_length,\n",
    "    max_target_length=max_target_length, \n",
    "    postprocess_fn=dataset_processor.dataset.postprocess_data\n",
    ")\n",
    "\n",
    "trainer, model = model_trainer.train_and_evaluate(train_dataset, eval_dataset, test_dataset)\n",
    "\n",
    "model.save_pretrained(model_save_path)\n",
    "dataset_processor.tokenizer.save_pretrained(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
