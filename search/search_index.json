{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Overview","text":"\ud83e\udd96 Turkish LM Tuner"},{"location":"index.html#overview","title":"Overview","text":"<p>Turkish LM Tuner is a library for fine-tuning Turkish language models on various NLP tasks. It is built on top of Hugging Face Transformers library. It supports finetuning with conditional generation and sequence classification tasks. The library is designed to be modular and extensible. It is easy to add new tasks and models. The library also provides data loaders for various Turkish NLP datasets.</p>"},{"location":"index.html#installation","title":"Installation","text":"<p>You can install <code>turkish-lm-tuner</code> via PyPI: </p> <pre><code>pip install turkish-lm-tuner\n</code></pre> <p>Alternatively, you can use the following command to install the library:</p> <pre><code>pip install git+https://github.com/boun-tabi-LMG/turkish-lm-tuner.git\n</code></pre>"},{"location":"index.html#model-support","title":"Model Support","text":"<p>Any Encoder or ConditionalGeneration model that is compatible with Hugging Face Transformers library can be used with Turkish LM Tuner. The following models are tested and supported.</p> <ul> <li>TURNA</li> <li>mT5</li> <li>mBART</li> <li>BERTurk</li> </ul>"},{"location":"index.html#task-and-dataset-support","title":"Task and Dataset Support","text":"Task Datasets Text Classification Product Reviews, TTC4900, Tweet Sentiment Natural Language Inference NLI_TR, SNLI_TR, MultiNLI_TR Semantic Textual Similarity STSb_TR Named Entity Recognition WikiANN, Milliyet NER Part-of-Speech Tagging BOUN, IMST Text Summarization TR News, MLSUM, Combined TR News and MLSUM Title Generation TR News, MLSUM, Combined TR News and MLSUM Paraphrase Generation OpenSubtitles, Tatoeba, TED Talks"},{"location":"index.html#usage","title":"Usage","text":"<p>The tutorials in the documentation can help you get started with <code>turkish-lm-tuner</code>.</p>"},{"location":"index.html#examples","title":"Examples","text":""},{"location":"index.html#fine-tune-and-evaluate-a-conditional-generation-model","title":"Fine-tune and evaluate a conditional generation model","text":"<pre><code>from turkish_lm_tuner import DatasetProcessor, TrainerForConditionalGeneration\n\ndataset_name = \"tr_news\"\ntask = \"summarization\"\ntask_format=\"conditional_generation\"\nmodel_name = \"boun-tabi-LMG/TURNA\"\nmax_input_length = 764\nmax_target_length = 128\ndataset_processor = DatasetProcessor(\n    dataset_name=dataset_name, task=task, task_format=task_format, task_mode='',\n    tokenizer_name=model_name, max_input_length=max_input_length, max_target_length=max_target_length\n)\n\ntrain_dataset = dataset_processor.load_and_preprocess_data(split='train')\neval_dataset = dataset_processor.load_and_preprocess_data(split='validation')\ntest_dataset = dataset_processor.load_and_preprocess_data(split=\"test\")\n\ntraining_params = {\n    'num_train_epochs': 10,\n    'per_device_train_batch_size': 4,\n    'per_device_eval_batch_size': 4,\n    'output_dir': './', \n    'evaluation_strategy': 'epoch',\n    'save_strategy': 'epoch',\n    'predict_with_generate': True    \n}\noptimizer_params = {\n    'optimizer_type': 'adafactor',\n    'scheduler': False,\n}\n\nmodel_trainer = TrainerForConditionalGeneration(\n    model_name=model_name, task=task,\n    optimizer_params=optimizer_params,\n    training_params=training_params,\n    model_save_path=\"turna_summarization_tr_news\",\n    max_input_length=max_input_length,\n    max_target_length=max_target_length, \n    postprocess_fn=dataset_processor.dataset.postprocess_data\n)\n\ntrainer, model = model_trainer.train_and_evaluate(train_dataset, eval_dataset, test_dataset)\n\nmodel.save_pretrained(model_save_path)\ndataset_processor.tokenizer.save_pretrained(model_save_path)\n</code></pre>"},{"location":"index.html#evaluate-a-conditional-generation-model-with-custom-generation-config","title":"Evaluate a conditional generation model with custom generation config","text":"<pre><code>from turkish_lm_tuner import DatasetProcessor, EvaluatorForConditionalGeneration\n\ndataset_name = \"tr_news\"\ntask = \"summarization\"\ntask_format=\"conditional_generation\"\nmodel_name = \"boun-tabi-LMG/TURNA\"\ntask_mode = ''\nmax_input_length = 764\nmax_target_length = 128\ndataset_processor = DatasetProcessor(\n    dataset_name, task, task_format, task_mode,\n    model_name, max_input_length, max_target_length\n)\n\ntest_dataset = dataset_processor.load_and_preprocess_data(split=\"test\")\n\ntest_params = {\n    'per_device_eval_batch_size': 4,\n    'output_dir': './',\n    'predict_with_generate': True\n}\n\nmodel_path = \"turna_tr_news_summarization\"\ngeneration_params = {\n    'num_beams': 4,\n    'length_penalty': 2.0,\n    'no_repeat_ngram_size': 3,\n    'early_stopping': True,\n    'max_length': 128,\n    'min_length': 30,\n}\nevaluator = EvaluatorForConditionalGeneration(\n    model_path, model_name, task, max_input_length, max_target_length, test_params,\n    generation_params, dataset_processor.dataset.postprocess_data\n)\nresults = evaluator.evaluate_model(test_dataset)\nprint(results)\n</code></pre>"},{"location":"index.html#reference","title":"Reference","text":"<p>If you use this repository, please cite the following related paper:</p> <pre><code>@inproceedings{uludogan-etal-2024-turna,\n    title = \"{TURNA}: A {T}urkish Encoder-Decoder Language Model for Enhanced Understanding and Generation\",\n    author = {Uludo{\\u{g}}an, G{\\\"o}k{\\c{c}}e  and\n      Balal, Zeynep  and\n      Akkurt, Furkan  and\n      Turker, Meliksah  and\n      Gungor, Onur  and\n      {\\\"U}sk{\\\"u}darl{\\i}, Susan},\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.600\",\n    doi = \"10.18653/v1/2024.findings-acl.600\",\n    pages = \"10103--10117\",\n}\n</code></pre>"},{"location":"index.html#license","title":"License","text":"<p>Note that all datasets belong to their respective owners. If you use the datasets provided by this library, please cite the original source.</p> <p>This code base is licensed under the MIT license. See LICENSE for details.</p>"},{"location":"license.html","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2023 boun-tabi-LMG</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"tutorials/evaluation.html","title":"Evaluation","text":""},{"location":"tutorials/evaluation.html#how-to-use-turkish-lm-tuner-to-evaluate-your-model","title":"How to use <code>turkish-lm-tuner</code> to evaluate your model\u00b6","text":"<p><code>turkish-lm-tuner</code> provides task specific metrics and evaluator for easier evaluation of fine-tuned language models.</p>"},{"location":"tutorials/evaluation.html#import-task-specific-metrics","title":"Import task specific metrics\u00b6","text":"<p><code>Evaluator</code> class provides task specific metrics for tasks. It takes two arguments: <code>task</code> and <code>metrics</code>. The supported tasks are:</p> <ul> <li><code>classification</code></li> <li><code>summarization</code></li> <li><code>paraphrasing</code></li> <li><code>title_generation</code></li> <li><code>nli</code></li> <li><code>semantic_similarity</code></li> <li><code>ner</code></li> <li><code>pos_tagging</code></li> </ul> <p>The supported metrics are:</p> <ul> <li><code>accuracy</code></li> <li><code>precision</code></li> <li><code>precision_weighted</code></li> <li><code>recall</code></li> <li><code>recall_weighted</code></li> <li><code>f1</code></li> <li><code>f1_macro</code></li> <li><code>f1_micro</code></li> <li><code>f1_weighted</code></li> <li><code>pearsonr</code></li> <li><code>bleu</code></li> <li><code>meteor</code></li> <li><code>rouge</code></li> <li><code>ter</code></li> <li><code>squad</code></li> <li><code>seqeval</code></li> </ul> <p>For example, to import metrics for <code>classification</code> task:</p> <pre>from turkish_lm_tuner import Evaluator\n\neval = Evaluator(task='classification')\n</pre>"},{"location":"tutorials/evaluation.html#compute-metrics","title":"Compute metrics\u00b6","text":"<p>Metrics are then computed by calling <code>compute_metrics</code> method of <code>Evaluator</code> class. <code>compute_metrics</code> method takes two arguments: <code>preds</code> and <code>labels</code>. <code>labels</code> is the ground truth labels and <code>preds</code> is the predicted labels.</p> <p>For example, to compute metrics for <code>classification</code> task:</p> <pre>eval.compute_metrics([0, 0, 1, 1], [1, 0, 1, 1])\n</pre>"},{"location":"tutorials/finetuning.html","title":"Finetuning","text":"In\u00a0[\u00a0]: Copied! <pre>from turkish_lm_tuner import DatasetProcessor\n\ndataset_name = \"tr_news\" \ntask = \"summarization\"\ntask_mode = '' # either '', '[NLU]', '[NLG]', '[S2S]'\ntask_format=\"conditional_generation\"\nmodel_name = \"boun-tabi-LMG/TURNA\"\nmax_input_length = 764\nmax_target_length = 128\n\n\ndataset_processor = DatasetProcessor(\n        dataset_name=dataset_name, task=task, task_format=task_format, task_mode=task_mode,\n        tokenizer_name=model_name, max_input_length=max_input_length, max_target_length=max_target_length\n)\n\ntrain_dataset = dataset_processor.load_and_preprocess_data(split='train')\neval_dataset = dataset_processor.load_and_preprocess_data(split='validation')\ntest_dataset = dataset_processor.load_and_preprocess_data(split=\"test\")\n</pre> from turkish_lm_tuner import DatasetProcessor  dataset_name = \"tr_news\"  task = \"summarization\" task_mode = '' # either '', '[NLU]', '[NLG]', '[S2S]' task_format=\"conditional_generation\" model_name = \"boun-tabi-LMG/TURNA\" max_input_length = 764 max_target_length = 128   dataset_processor = DatasetProcessor(         dataset_name=dataset_name, task=task, task_format=task_format, task_mode=task_mode,         tokenizer_name=model_name, max_input_length=max_input_length, max_target_length=max_target_length )  train_dataset = dataset_processor.load_and_preprocess_data(split='train') eval_dataset = dataset_processor.load_and_preprocess_data(split='validation') test_dataset = dataset_processor.load_and_preprocess_data(split=\"test\") In\u00a0[\u00a0]: Copied! <pre>training_params = {\n    'num_train_epochs': 10\n    'per_device_train_batch_size': 4,\n    'per_device_eval_batch_size': 4,\n    'output_dir': './', \n    'evaluation_strategy': 'epoch',\n    'save_strategy': 'epoch',\n    'predict_with_generate': True    \n}\noptimizer_params = {\n    'optimizer_type': 'adafactor',\n    'scheduler': False,\n}\n</pre>  training_params = {     'num_train_epochs': 10     'per_device_train_batch_size': 4,     'per_device_eval_batch_size': 4,     'output_dir': './',      'evaluation_strategy': 'epoch',     'save_strategy': 'epoch',     'predict_with_generate': True     } optimizer_params = {     'optimizer_type': 'adafactor',     'scheduler': False, } In\u00a0[\u00a0]: Copied! <pre>model_trainer = TrainerForConditionalGeneration(\n    model_name=model_name, task=task,\n    training_params=training_params,\n    optimizer_params=optimizer_params,\n    model_save_path=\"turna_summarization_tr_news\",\n    max_input_length=max_input_length,\n    max_target_length=max_target_length, \n    postprocess_fn=dataset_processor.dataset.postprocess_data\n)\n\ntrainer, model = model_trainer.train_and_evaluate(train_dataset, eval_dataset, test_dataset)\n\nmodel.save_pretrained(model_save_path)\ndataset_processor.tokenizer.save_pretrained(model_save_path)\n</pre>  model_trainer = TrainerForConditionalGeneration(     model_name=model_name, task=task,     training_params=training_params,     optimizer_params=optimizer_params,     model_save_path=\"turna_summarization_tr_news\",     max_input_length=max_input_length,     max_target_length=max_target_length,      postprocess_fn=dataset_processor.dataset.postprocess_data )  trainer, model = model_trainer.train_and_evaluate(train_dataset, eval_dataset, test_dataset)  model.save_pretrained(model_save_path) dataset_processor.tokenizer.save_pretrained(model_save_path)"},{"location":"tutorials/finetuning.html#how-to-use-turkish-lm-tuner-to-finetune-an-encoder-decoder-language-model-on-summarization-task","title":"How to use <code>turkish-lm-tuner</code> to finetune an encoder-decoder language model on summarization task\u00b6","text":"<p><code>turkish-lm-tuner</code> is a library designed for finetuning a language model on specific datasets. It is based on transformers. It is designed to be easy to use and to work with any encoder and encoder-decoder language model that is supported by <code>transformers</code>.</p> <p><code>turkish-lm-tuner</code> supports both encoder and encoder-decoder models. It offers wrappers for various task datasets (like paraphrasing, text classification, summarization, etc.).  Additionally, it includes easily importable and usable evaluation metrics for various tasks.</p> <p>In this tutorial, we will show how to use <code>turkish-lm-tuner</code> to finetune an encoder-decoder language model on summarization task. We will use TR News dataset for fine-tuning TURNA model.</p>"},{"location":"tutorials/finetuning.html#installation","title":"Installation\u00b6","text":"<p>The library can be installed as follows:</p> <pre>pip install turkish-lm-tuner\n</pre>"},{"location":"tutorials/finetuning.html#importing-and-processing-dataset","title":"Importing and processing dataset\u00b6","text":"<p>The library includes wrappers for various datasets. These wrappers can be used to import the dataset and to preprocess it for finetuning.</p> <p>For summarization task, we will use TR News dataset. This dataset includes news articles and their summaries. We will use the wrapper for this dataset to import and preprocess it based on the task configuration.</p>"},{"location":"tutorials/finetuning.html#preparing-finetuning-parameters","title":"Preparing finetuning parameters\u00b6","text":""},{"location":"tutorials/finetuning.html#finetuning-the-model","title":"Finetuning the model\u00b6","text":""},{"location":"tutorials/getting-started.html","title":"Getting-Started","text":""},{"location":"tutorials/getting-started.html#getting-started","title":"Getting-Started\u00b6","text":"<p><code>turkish-lm-tuner</code> simplifies the process of finetuning and evaluating transformer language models on various NLP tasks, with a special focus on Turkish language datasets. It is built on top of the transformers and supports both encoder and encoder-decoder models.</p>"},{"location":"tutorials/getting-started.html#key-features","title":"Key Features\u00b6","text":"<ul> <li>Support for Multiple Tasks: Includes wrappers for tasks like summarization, text classification, and more.</li> <li>Easy Dataset Import and Processing: Utilities for importing and preprocessing datasets tailored for specific NLP tasks.</li> <li>Simple Model Finetuning: Streamlined finetuning of models with customizable parameters.</li> <li>Comprehensive Evaluation Metrics: Offers a wide range of metrics for different tasks, making evaluation straightforward.</li> </ul>"},{"location":"tutorials/getting-started.html#installation","title":"Installation\u00b6","text":"<p><code>turkish-lm-tuner</code> can be installed as follows:</p> <pre>pip install turkish-lm-tuner\n</pre>"},{"location":"tutorials/getting-started.html#finetuning","title":"Finetuning\u00b6","text":""},{"location":"tutorials/getting-started.html#example-finetuning-turna-on-tr-news-dataset","title":"Example: Finetuning TURNA on TR News Dataset\u00b6","text":""},{"location":"tutorials/getting-started.html#importing-and-processing-the-dataset","title":"Importing and Processing the Dataset\u00b6","text":"<pre>from turkish_lm_tuner import DatasetProcessor\n\n# Define parameters\ndataset_name = \"tr_news\"\ntask = \"summarization\"\ntask_format = \"conditional_generation\"\nmodel_name = \"boun-tabi-LMG/TURNA\"\nmax_input_length = 764\nmax_target_length = 128\n\n# Initialize and process dataset\ndataset_processor = DatasetProcessor(dataset_name, task, task_format, '', model_name, max_input_length, max_target_length)\ntrain_dataset = dataset_processor.load_and_preprocess_data('train')\neval_dataset = dataset_processor.load_and_preprocess_data('validation')\n</pre>"},{"location":"tutorials/getting-started.html#setting-up-training-parameters-and-finetuning","title":"Setting up Training Parameters and Finetuning\u00b6","text":"<pre>from turkish_lm_tuner import TrainerForConditionalGeneration\n\n# Define training and optimizer parameters\ntraining_params = {\n    'num_train_epochs': 10,\n    'per_device_train_batch_size': 4,\n    'per_device_eval_batch_size': 4,\n    'output_dir': './',\n    'evaluation_strategy': 'epoch',\n    'save_strategy': 'epoch',\n    'predict_with_generate': True\n}\noptimizer_params = {\n    'optimizer_type': 'adafactor',\n    'scheduler': False\n}\nmodel_save_path = \"turna_summarization_tr_news\"\n\n# Finetuning the model\nmodel_trainer = TrainerForConditionalGeneration(model_name, task, training_params, optimizer_params, model_save_path, max_input_length, max_target_length, dataset_processor.dataset.postprocess_data)\ntrainer, model = model_trainer.train_and_evaluate(train_dataset, eval_dataset, None)\n\n# Save the model\nmodel.save_pretrained(model_save_path)\ndataset_processor.tokenizer.save_pretrained(model_save_path)\n</pre>"},{"location":"tutorials/getting-started.html#evaluation","title":"Evaluation\u00b6","text":""},{"location":"tutorials/getting-started.html#example-using-evaluator-for-the-summarization-task","title":"Example: Using Evaluator for the Summarization Task\u00b6","text":""},{"location":"tutorials/getting-started.html#importing-task-specific-metrics","title":"Importing Task-Specific Metrics\u00b6","text":"<pre>from turkish_lm_tuner import Evaluator\n\n# Initialize evaluator for summarization task\neval = Evaluator(task='summarization')\n</pre>"},{"location":"tutorials/getting-started.html#computing-metrics","title":"Computing Metrics\u00b6","text":"<pre># Example predictions and labels\npreds = [generated_summary1, generated_summary2]\nlabels = [true_summary1, true_summary2]\n\n# Compute metrics\nresults = eval.compute_metrics(preds, labels)\n</pre>"},{"location":"tutorials/getting-started.html#contributing","title":"Contributing\u00b6","text":"<p>We welcome contributions to turkish-lm-tuner! Whether it's improving documentation, adding new features, or reporting issues, your input is valuable.</p>"},{"location":"tutorials/inference.html","title":"Inference with","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install turkish-lm-tuner\n</pre> !pip install turkish-lm-tuner In\u00a0[\u00a0]: Copied! <pre>from turkish_lm_tuner import TextPredictor\npredictor = TextPredictor(model_name='boun-tabi-LMG/TURNA', task='generation')\npredictor.predict('[S2S] Bir varm\u0131\u015f, bir yokmu\u015f, evvel zaman i\u00e7inde, kalbur saman i\u00e7inde, uzak diyarlar\u0131n birinde bir turna&lt;EOS&gt;')\n</pre> from turkish_lm_tuner import TextPredictor predictor = TextPredictor(model_name='boun-tabi-LMG/TURNA', task='generation') predictor.predict('[S2S] Bir varm\u0131\u015f, bir yokmu\u015f, evvel zaman i\u00e7inde, kalbur saman i\u00e7inde, uzak diyarlar\u0131n birinde bir turna') In\u00a0[\u00a0]: Copied! <pre>from turkish_lm_tuner import TextPredictor\npredictor = TextPredictor(model_name='boun-tabi-LMG/turna_ner_milliyet', task='ner')\npredictor.predict('Ecevit, Irak h\u00fck\u00fcmetinin de Ankara B\u00fcy\u00fckel\u00e7ili\u011fi i\u00e7in agreman istedi\u011fini belirtti.')\n</pre> from turkish_lm_tuner import TextPredictor predictor = TextPredictor(model_name='boun-tabi-LMG/turna_ner_milliyet', task='ner') predictor.predict('Ecevit, Irak h\u00fck\u00fcmetinin de Ankara B\u00fcy\u00fckel\u00e7ili\u011fi i\u00e7in agreman istedi\u011fini belirtti.') In\u00a0[\u00a0]: Copied! <pre>from turkish_lm_tuner import TextPredictor\npredictor = TextPredictor(model_name='boun-tabi-LMG/turna_summarization_tr_news', task='summarization')\npredictor.predict('Kalp krizi ge\u00e7irenlerin yakla\u015f\u0131k \u00fc\u00e7te birinin k\u0131sa bir s\u00fcre \u00f6nce grip atlatt\u0131\u011f\u0131 d\u00fc\u015f\u00fcn\u00fcl\u00fcyor. Peki grip vir\u00fcs\u00fc ne yap\u0131yor da kalp krizine yol a\u00e7\u0131yor? Karpuz \u015f\u00f6yle a\u00e7\u0131klad\u0131: Grip vir\u00fcs\u00fc kan\u0131n yap\u0131\u015fkanl\u0131\u011f\u0131n\u0131 veya p\u0131ht\u0131la\u015fmas\u0131n\u0131 art\u0131r\u0131yor.')\n</pre> from turkish_lm_tuner import TextPredictor predictor = TextPredictor(model_name='boun-tabi-LMG/turna_summarization_tr_news', task='summarization') predictor.predict('Kalp krizi ge\u00e7irenlerin yakla\u015f\u0131k \u00fc\u00e7te birinin k\u0131sa bir s\u00fcre \u00f6nce grip atlatt\u0131\u011f\u0131 d\u00fc\u015f\u00fcn\u00fcl\u00fcyor. Peki grip vir\u00fcs\u00fc ne yap\u0131yor da kalp krizine yol a\u00e7\u0131yor? Karpuz \u015f\u00f6yle a\u00e7\u0131klad\u0131: Grip vir\u00fcs\u00fc kan\u0131n yap\u0131\u015fkanl\u0131\u011f\u0131n\u0131 veya p\u0131ht\u0131la\u015fmas\u0131n\u0131 art\u0131r\u0131yor.') In\u00a0[\u00a0]: Copied! <pre>from turkish_lm_tuner import TextPredictor\npredictor = TextPredictor(model_name='boun-tabi-LMG/turna_classification_ttc4900', task='categorization')\npredictor.predict('anadolu_efes e 18 lik star ! beko_basketbol_ligi nde iddial\u0131 bir kadroyla sezona giren anadolu_efes transfer harekat\u0131na devam ediyor avrupa bas\u0131n\u0131nda yer alan iddialara g\u00f6re lacivert beyazl\u0131lar\u0131n son hedefi kk zagreb de forma giyen 1994 do\u011fumlu dario saric h\u0131rvat oyuncunun anadolu_efes ile kesin anla\u015fmaya vard\u0131\u011f\u0131 iddia edilirken efes in bu transfer i\u00e7in kk zagreb e 550 bin euro \u00f6deyece\u011fi ifade edildi saric in sezon sonuna kadar \u015fu anda kiral\u0131k olarak formas\u0131n\u0131 giydi\u011fi kk split te kalaca\u011f\u0131 ve sezon sonunda anadolu_efes e kat\u0131laca\u011f\u0131 belirtildi h\u0131rvat basketbolunun gelecek vaadeden isimlerinden biri olarak g\u00f6sterilen saric 2 05 boyunda ve k\u0131sa forvet pozisyonunda g\u00f6rev yap\u0131yor y\u0131ld\u0131z basketbolcu 2012 18 ya\u015f alt\u0131 avrupa_basketbol_\u015fampiyonas\u0131nda h\u0131rvatistan \u0131 \u015fampiyonlu\u011fa ta\u015f\u0131m\u0131\u015ft\u0131 final kar\u015f\u0131la\u015fmas\u0131nda litvanya potas\u0131na 39 say\u0131 b\u0131rakan saric turnuvay\u0131 25 6 say\u0131 10 1 ribaund ve 3 3 asist ortalamas\u0131yla tamamlam\u0131\u015ft\u0131')\n</pre> from turkish_lm_tuner import TextPredictor predictor = TextPredictor(model_name='boun-tabi-LMG/turna_classification_ttc4900', task='categorization') predictor.predict('anadolu_efes e 18 lik star ! beko_basketbol_ligi nde iddial\u0131 bir kadroyla sezona giren anadolu_efes transfer harekat\u0131na devam ediyor avrupa bas\u0131n\u0131nda yer alan iddialara g\u00f6re lacivert beyazl\u0131lar\u0131n son hedefi kk zagreb de forma giyen 1994 do\u011fumlu dario saric h\u0131rvat oyuncunun anadolu_efes ile kesin anla\u015fmaya vard\u0131\u011f\u0131 iddia edilirken efes in bu transfer i\u00e7in kk zagreb e 550 bin euro \u00f6deyece\u011fi ifade edildi saric in sezon sonuna kadar \u015fu anda kiral\u0131k olarak formas\u0131n\u0131 giydi\u011fi kk split te kalaca\u011f\u0131 ve sezon sonunda anadolu_efes e kat\u0131laca\u011f\u0131 belirtildi h\u0131rvat basketbolunun gelecek vaadeden isimlerinden biri olarak g\u00f6sterilen saric 2 05 boyunda ve k\u0131sa forvet pozisyonunda g\u00f6rev yap\u0131yor y\u0131ld\u0131z basketbolcu 2012 18 ya\u015f alt\u0131 avrupa_basketbol_\u015fampiyonas\u0131nda h\u0131rvatistan \u0131 \u015fampiyonlu\u011fa ta\u015f\u0131m\u0131\u015ft\u0131 final kar\u015f\u0131la\u015fmas\u0131nda litvanya potas\u0131na 39 say\u0131 b\u0131rakan saric turnuvay\u0131 25 6 say\u0131 10 1 ribaund ve 3 3 asist ortalamas\u0131yla tamamlam\u0131\u015ft\u0131') In\u00a0[\u00a0]: Copied! <pre>from turkish_lm_tuner import TextPredictor\npredictor = TextPredictor(model_name='boun-tabi-LMG/turna_nli_nli_tr', task='nli')\nhypothesis = \"Temple Bar'da \u00e7ok sanat\u00e7\u0131 var.\"\npremise = \"Temple Bar herhangi bir m\u00fczisyen veya sanat\u00e7\u0131dan tamamen yoksundur.\"\npredictor.predict(f\"hipotez: {hypothesis} \u00f6nerme: {premise}\")\n</pre> from turkish_lm_tuner import TextPredictor predictor = TextPredictor(model_name='boun-tabi-LMG/turna_nli_nli_tr', task='nli') hypothesis = \"Temple Bar'da \u00e7ok sanat\u00e7\u0131 var.\" premise = \"Temple Bar herhangi bir m\u00fczisyen veya sanat\u00e7\u0131dan tamamen yoksundur.\" predictor.predict(f\"hipotez: {hypothesis} \u00f6nerme: {premise}\") In\u00a0[\u00a0]: Copied! <pre>from turkish_lm_tuner import TextPredictor\npredictor = TextPredictor(model_name='boun-tabi-LMG/turna_classification_17bintweet_sentiment', task='sentiment')\npredictor.predict('sonunda bug\u00fcn kurtuldum senden')\n</pre> from turkish_lm_tuner import TextPredictor predictor = TextPredictor(model_name='boun-tabi-LMG/turna_classification_17bintweet_sentiment', task='sentiment') predictor.predict('sonunda bug\u00fcn kurtuldum senden') In\u00a0[\u00a0]: Copied! <pre>from turkish_lm_tuner import TextPredictor\npredictor = TextPredictor(model_name='boun-tabi-LMG/turna_classification_tr_product_reviews', task='sentiment')\npredictor.predict('Bu kadar iyi bir \u00fcr\u00fcn kullanmad\u0131m!')\n</pre> from turkish_lm_tuner import TextPredictor predictor = TextPredictor(model_name='boun-tabi-LMG/turna_classification_tr_product_reviews', task='sentiment') predictor.predict('Bu kadar iyi bir \u00fcr\u00fcn kullanmad\u0131m!') In\u00a0[\u00a0]: Copied! <pre>from turkish_lm_tuner import TextPredictor\npredictor = TextPredictor(model_name='boun-tabi-LMG/turna_pos_boun', task='pos_tagging')\npredictor.predict('\u00c7\u00fcnk\u00fc her ki\u015finin bir ba\u015fka yolu, bir ba\u015fka y\u00f6ntemi olmak gerektir.')\n</pre> from turkish_lm_tuner import TextPredictor predictor = TextPredictor(model_name='boun-tabi-LMG/turna_pos_boun', task='pos_tagging') predictor.predict('\u00c7\u00fcnk\u00fc her ki\u015finin bir ba\u015fka yolu, bir ba\u015fka y\u00f6ntemi olmak gerektir.') In\u00a0[\u00a0]: Copied! <pre>from turkish_lm_tuner import TextPredictor\npredictor = TextPredictor(model_name='boun-tabi-LMG/turna_paraphrasing_tatoeba', task='paraphrasing')\npredictor.predict('Kalp krizi ge\u00e7irenlerin yakla\u015f\u0131k \u00fc\u00e7te birinin k\u0131sa bir s\u00fcre \u00f6nce grip atlatt\u0131\u011f\u0131 d\u00fc\u015f\u00fcn\u00fcl\u00fcyor. ')\n</pre> from turkish_lm_tuner import TextPredictor predictor = TextPredictor(model_name='boun-tabi-LMG/turna_paraphrasing_tatoeba', task='paraphrasing') predictor.predict('Kalp krizi ge\u00e7irenlerin yakla\u015f\u0131k \u00fc\u00e7te birinin k\u0131sa bir s\u00fcre \u00f6nce grip atlatt\u0131\u011f\u0131 d\u00fc\u015f\u00fcn\u00fcl\u00fcyor. ') In\u00a0[\u00a0]: Copied! <pre>from turkish_lm_tuner import TextPredictor\npredictor = TextPredictor(model_name='boun-tabi-LMG/turna_title_generation_mlsum', task='title_generation')\npredictor.predict('Kalp krizi ge\u00e7irenlerin yakla\u015f\u0131k \u00fc\u00e7te birinin k\u0131sa bir s\u00fcre \u00f6nce grip atlatt\u0131\u011f\u0131 d\u00fc\u015f\u00fcn\u00fcl\u00fcyor. Peki grip vir\u00fcs\u00fc ne yap\u0131yor da kalp krizine yol a\u00e7\u0131yor? Karpuz \u015f\u00f6yle a\u00e7\u0131klad\u0131: Grip vir\u00fcs\u00fc kan\u0131n yap\u0131\u015fkanl\u0131\u011f\u0131n\u0131 veya p\u0131ht\u0131la\u015fmas\u0131n\u0131 art\u0131r\u0131yor.')\n</pre> from turkish_lm_tuner import TextPredictor predictor = TextPredictor(model_name='boun-tabi-LMG/turna_title_generation_mlsum', task='title_generation') predictor.predict('Kalp krizi ge\u00e7irenlerin yakla\u015f\u0131k \u00fc\u00e7te birinin k\u0131sa bir s\u00fcre \u00f6nce grip atlatt\u0131\u011f\u0131 d\u00fc\u015f\u00fcn\u00fcl\u00fcyor. Peki grip vir\u00fcs\u00fc ne yap\u0131yor da kalp krizine yol a\u00e7\u0131yor? Karpuz \u015f\u00f6yle a\u00e7\u0131klad\u0131: Grip vir\u00fcs\u00fc kan\u0131n yap\u0131\u015fkanl\u0131\u011f\u0131n\u0131 veya p\u0131ht\u0131la\u015fmas\u0131n\u0131 art\u0131r\u0131yor.') In\u00a0[\u00a0]: Copied! <pre>from turkish_lm_tuner import TextPredictor\npredictor = TextPredictor(model_name='boun-tabi-LMG/turna_semantic_similarity_stsb_tr', task='sts')\nfirst_text = \"Bug\u00fcn okula gitmedim.\"\nsecond_text = \"Ben okula gitmedim bug\u00fcn.\"\npredictor.predict(f\"ilk c\u00fcmle: {first_text} ikinci c\u00fcmle: {second_text}\")\n</pre> from turkish_lm_tuner import TextPredictor predictor = TextPredictor(model_name='boun-tabi-LMG/turna_semantic_similarity_stsb_tr', task='sts') first_text = \"Bug\u00fcn okula gitmedim.\" second_text = \"Ben okula gitmedim bug\u00fcn.\" predictor.predict(f\"ilk c\u00fcmle: {first_text} ikinci c\u00fcmle: {second_text}\")"},{"location":"tutorials/inference.html#inference-with-turkish-lm-tuner","title":"Inference with <code>turkish-lm-tuner</code>\u00b6","text":""},{"location":"tutorials/inference.html#installation","title":"Installation\u00b6","text":"<p>The library can be installed as follows:</p>"},{"location":"tutorials/inference.html#how-to-use-turkish-lm-tuner-for-inference","title":"How to use <code>turkish-lm-tuner</code> for inference\u00b6","text":"<p><code>turkish-lm-tuner</code> allows easy inference on Turkish language models. It supports various tasks.</p> <p>There can be multiple available fine-tuned models for each task. Check out available models here.</p>"},{"location":"tutorials/inference.html#examples","title":"Examples\u00b6","text":"<p>The following examples show how to use <code>turkish-lm-tuner</code> for inference with conditional generation models.</p>"},{"location":"tutorials/inference.html#text-generation","title":"Text Generation\u00b6","text":""},{"location":"tutorials/inference.html#named-entity-recognition","title":"Named Entity Recognition\u00b6","text":""},{"location":"tutorials/inference.html#text-summarization","title":"Text Summarization\u00b6","text":""},{"location":"tutorials/inference.html#text-categorization","title":"Text Categorization\u00b6","text":""},{"location":"tutorials/inference.html#natural-language-inference","title":"Natural Language Inference\u00b6","text":""},{"location":"tutorials/inference.html#sentiment-classification","title":"Sentiment Classification\u00b6","text":"<p>Models for sentiment classification were fine-tuned on two separate datasets:</p> <ul> <li>Product Reviews, which contains two labels: \"negatif\" and \"pozitif\"</li> <li>Tweet Sentiment, which contains three labels: \"olumsuz\", \"n\u00f6tr\", and \"olumlu\"</li> </ul>"},{"location":"tutorials/inference.html#part-of-speech-tagging","title":"Part-of-speech Tagging\u00b6","text":""},{"location":"tutorials/inference.html#text-paraphrasing","title":"Text Paraphrasing\u00b6","text":""},{"location":"tutorials/inference.html#news-title-generation","title":"News Title Generation\u00b6","text":""},{"location":"tutorials/inference.html#semantic-textual-similarity","title":"Semantic Textual Similarity\u00b6","text":""}]}